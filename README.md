# get.ro - A news crawler and classifier

This project aims to replicate some of the functionality of Google News. We crawl thousands of sources, collect the new articles and extract the text and images. But by doing that, we end up with a huge mass of tens of thousands of news items every day which is impossible to browse by hand. So we must build a news classifier to split the firehose of news into manageable streams. And since it's possible, why classify just in 10-20 topics when we could have thousands? 

## News crawler

The first stage of our system is the crawler. The crawler has to start somewhere so we build a list of newspaper homepage URLs. In order to do that we shamelessly exploit other projects who index the Romanian newssites. Another method is to query Google with keywords that only appear in very recent news (such as names of people mentioned for the first time in the last 24 hours), identifying newspapers.

We crawl the homepages every hour and extract all the links, including from the RSS feed. We compare the links with a list of known older articles and skip those. Since the news homepages change all the time, in a few days we are able to filter out old news from recent ones.

Now that we have the HTML of news articles, we need to extract the title, text and main image. In order to do that we keep a database of all phrases ever met on the same newspaper, and filter out paragraphs that have been seen before. This method works well, but more recently there have been other methods that rank text inside HTML pages based on heuristics, they work well too and don't require the large database of past phrases.

## Classifying news

The main problem we have when we get to classifying news is that we don't actually have a labeled dataset. We don't even have a complete topic list. So we choose word embeddings as our main workhorse by learning unsupervised representations. This works great because we can now have rich representations of topics as well. We just need to select one single keyword or a few for each topic, no need to have thousands of hand-labeled examples.

We built word embeddings using many available tools - starting with word2vec, glove, fasttext and finally doc2vecC. The best embeddings for our purposes were the ones generated by doc2vecC because this agorithm models words the context.

In order to improve the efficiency of this method we treat collocations and names like single tokens and learn vectors for them like we learn all the rest of the vocabulary. Wikipedia was of invaluable help to us here. We simply matched the 


We have built a web tool to visualize word clouds by similarity and help us define topics in an interactive way.


    - main problem: don't have a dataset, using unsupervised methods and one-shot learning
    - word embeddings
        - my word embeddings tool
        - word2vec, fasttext, doc2vecC
        - collocations and entity names
            - counting ngrams with count-min-sketch
        - exploring word vectors
            - online tool
            
    - topic definitions
        - how I built the topic list -> clustering
        - tool to edit topics
            - problems with topic overlap
        - finding "strong words"
    - document embeddings
        - attention schemes - tf-idf, word similarity based
        - combining vocabulary check with document embeddings
    - ranking
        - finding trends by comparing word distributions

## Rendering the website
    - inverted index & document vectors
        - divided by hour, rebuiding only the last period
    - generating the website
